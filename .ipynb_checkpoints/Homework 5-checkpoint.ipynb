{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eb988da-51d0-4949-b9ca-7a8d95e7b3c5",
   "metadata": {},
   "source": [
    "# Homework 5\n",
    "### Rolando Santos\n",
    "### https://github.com/rsantos2032/DSCI401/\n",
    "### 2023-11-01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ce76c3-b170-4544-a5b0-49323d6183d9",
   "metadata": {},
   "source": [
    "### Question 1: Write no more than TWO paragraphs discussing the issues arising from each of these scenarios:\r",
    "\r",
    "#### a. In 2006, AOL released a database of search terms that users had used in the prior month (see http://www.nytimes.com/2006/08/09/technology/09aol.html). Research this disclosure and the reaction that ensued. What ethical issues are involved? What potential impact has this disclosure had?\r",
    "\r",
    "After reading the article it seems that a big issue was that (and especially at the time), people revealed a lot about themselves in their search histories, and in the case of Thelma Arnold, people were able to track down who she actually was. What AOL did was a breach in privacy with their users, as most did not assume that their searches were being stored in a database let alone to be released towards the general public. Despite the user's real name and information was disclosed in the database the entries had an associated user ID, i.e. NO. 4417749 for Ms. Thelma Arnold, associated with them. This means people could take all the entries for a specific user and use it to pinpoint who they are or their general vicinity, especially if they notice something interesting about that user. This data could even potentially be used for blackmail, if a bad actor were to compile enough of AOLs data to identify the person using their queries and saw that they had \"scandalous\" search queries, they could use this information to threaten that user. AOL realized how much issues this situation created and took down the information from their site shortly after they released it. Although the idea was to have this data be released for academic research reasons, I believe releasing the data without the consent of the users and with user IDs attached to the data was not the correct move.\r",
    "\r",
    "#### b. A Slate article (http://tinyurl.com/slate-ethics) discussed whether race/ethnicity should be included in a predictive model for how long a homeless family would stay in homeless services. Discuss the ethical considerations involved in whether race/ethnicity should be included as a predictor in the model.\r",
    "\r",
    "One of the ethical issues, as mentioned by the article, is that the dataset presented for training was historical data over the last 30 years which is extremely susceptible to bias. I believe that it is better off to not have race/ethnicity included in the training model, even though it compromises potential model accuracies. Especially considering this algorithm was used to pair homeless families with an appropriate service, the bias from the dataset could add an additional layer of inequality, the writer of the article mentioned that at a glance there was a trend between race and lack of services. If this algorithm were to be used to help out individuals, removing race would remove some bias from the model, making the model closer to one that evaluates based on other traits with less bias. However, its difficult to make a model that is free from bias, other variables such as location may also present some bias due to segregation found in places like NYC.\r",
    "\r",
    "#### c. A company uses a machine-learning algorithm to determine which job advertisement to display for users searching for technology jobs. Based on past results, the algorithm tends to display lower-paying jobs for women than for men (after controlling for other characteristics than gender). What ethical considerations might be considered when reviewing this algorithm?\r",
    "\r",
    "The main ethical concern with regards to this algorithm is that it prevents women from having the same job oppurtunities as their male peers. The algorithm appears to have bias present in the training data which is causing this issue. I believe that the company should re-evaluate their algorithm and training data so that such biases are minimized. The company could also face some sort of legal ramifications if these details are made public. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
